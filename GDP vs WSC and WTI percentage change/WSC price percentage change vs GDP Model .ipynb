{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WCS Price percentage change vs GDP Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cleaned Data Csv/Merged_GDP_WCS_Data.csv'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "#Define file paths for the uploaded datasets\n",
    "gdp_file = \"Cleaned Data Csv/Canada_USA_GDP_2014-2024.csv\"\n",
    "wcs_file = \"Cleaned Data Csv/WCS_Oil_Prices_Year_Only.csv\"\n",
    "\n",
    "# Load datasets\n",
    "gdp_df = pd.read_csv(gdp_file)\n",
    "wcs_df = pd.read_csv(wcs_file)\n",
    "\n",
    "# Remove unnecessary columns (such as unnamed index columns if present)\n",
    "for df in [gdp_df, wcs_df]:\n",
    "    df.drop(columns=[col for col in df.columns if \"Unnamed\" in col], inplace=True, errors='ignore')\n",
    "\n",
    "# Standardize column names by stripping spaces and converting to lowercase\n",
    "for df in [gdp_df, wcs_df]:\n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "# Ensure 'year' column exists and convert to integer\n",
    "for df in [gdp_df, wcs_df]:\n",
    "    if \"year\" in df.columns:\n",
    "        df[\"year\"] = pd.to_numeric(df[\"year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Clean WCS percent change column (remove '%' and convert to float if it exists)\n",
    "if \"percentchange\" in wcs_df.columns:\n",
    "    wcs_df[\"percentchange\"] = wcs_df[\"percentchange\"].str.replace(\"%\", \"\", regex=True).astype(float)\n",
    "\n",
    "# Rename columns for clarity (if they exist)\n",
    "gdp_df.rename(columns={\"gdp per capita (constant 2015 us$)\": \"gdp_per_capita\"}, inplace=True)\n",
    "wcs_df.rename(columns={\"price\": \"wcs_price\", \"percentchange\": \"wcs_percent_change\"}, inplace=True)\n",
    "\n",
    "# Remove duplicates\n",
    "for df in [gdp_df, wcs_df]:\n",
    "    df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Drop any remaining NaN values\n",
    "for df in [gdp_df, wcs_df]:\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "# Merge both datasets on the year column\n",
    "merged_df = gdp_df.merge(wcs_df, on=\"year\", how=\"inner\")\n",
    "\n",
    "# Save merged dataset to a CSV file for download\n",
    "merged_file_path = \"Cleaned Data Csv/Merged_GDP_WCS_Data.csv\"\n",
    "merged_df.to_csv(merged_file_path, index=False)\n",
    "\n",
    "# Provide download link\n",
    "merged_file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country name</th>\n",
       "      <th>country code</th>\n",
       "      <th>year</th>\n",
       "      <th>gdp_per_capita</th>\n",
       "      <th>type_</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Canada</td>\n",
       "      <td>CAN</td>\n",
       "      <td>2014</td>\n",
       "      <td>43643.24</td>\n",
       "      <td>WCS</td>\n",
       "      <td>65.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>United States</td>\n",
       "      <td>USA</td>\n",
       "      <td>2014</td>\n",
       "      <td>55817.56</td>\n",
       "      <td>WCS</td>\n",
       "      <td>65.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Canada</td>\n",
       "      <td>CAN</td>\n",
       "      <td>2015</td>\n",
       "      <td>43594.19</td>\n",
       "      <td>WCS</td>\n",
       "      <td>30.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>United States</td>\n",
       "      <td>USA</td>\n",
       "      <td>2015</td>\n",
       "      <td>57040.21</td>\n",
       "      <td>WCS</td>\n",
       "      <td>30.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Canada</td>\n",
       "      <td>CAN</td>\n",
       "      <td>2016</td>\n",
       "      <td>43551.34</td>\n",
       "      <td>WCS</td>\n",
       "      <td>17.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>United States</td>\n",
       "      <td>USA</td>\n",
       "      <td>2016</td>\n",
       "      <td>57658.67</td>\n",
       "      <td>WCS</td>\n",
       "      <td>17.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Canada</td>\n",
       "      <td>CAN</td>\n",
       "      <td>2017</td>\n",
       "      <td>44339.39</td>\n",
       "      <td>WCS</td>\n",
       "      <td>37.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>United States</td>\n",
       "      <td>USA</td>\n",
       "      <td>2017</td>\n",
       "      <td>58703.14</td>\n",
       "      <td>WCS</td>\n",
       "      <td>37.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Canada</td>\n",
       "      <td>CAN</td>\n",
       "      <td>2018</td>\n",
       "      <td>44907.34</td>\n",
       "      <td>WCS</td>\n",
       "      <td>42.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>United States</td>\n",
       "      <td>USA</td>\n",
       "      <td>2018</td>\n",
       "      <td>60127.21</td>\n",
       "      <td>WCS</td>\n",
       "      <td>42.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Canada</td>\n",
       "      <td>CAN</td>\n",
       "      <td>2019</td>\n",
       "      <td>45100.29</td>\n",
       "      <td>WCS</td>\n",
       "      <td>34.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>United States</td>\n",
       "      <td>USA</td>\n",
       "      <td>2019</td>\n",
       "      <td>61400.55</td>\n",
       "      <td>WCS</td>\n",
       "      <td>34.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Canada</td>\n",
       "      <td>CAN</td>\n",
       "      <td>2020</td>\n",
       "      <td>42366.13</td>\n",
       "      <td>WCS</td>\n",
       "      <td>36.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>United States</td>\n",
       "      <td>USA</td>\n",
       "      <td>2020</td>\n",
       "      <td>59493.15</td>\n",
       "      <td>WCS</td>\n",
       "      <td>36.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Canada</td>\n",
       "      <td>CAN</td>\n",
       "      <td>2021</td>\n",
       "      <td>44359.62</td>\n",
       "      <td>WCS</td>\n",
       "      <td>40.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>United States</td>\n",
       "      <td>USA</td>\n",
       "      <td>2021</td>\n",
       "      <td>62996.29</td>\n",
       "      <td>WCS</td>\n",
       "      <td>40.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Canada</td>\n",
       "      <td>CAN</td>\n",
       "      <td>2022</td>\n",
       "      <td>45227.14</td>\n",
       "      <td>WCS</td>\n",
       "      <td>65.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>United States</td>\n",
       "      <td>USA</td>\n",
       "      <td>2022</td>\n",
       "      <td>64342.12</td>\n",
       "      <td>WCS</td>\n",
       "      <td>65.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Canada</td>\n",
       "      <td>CAN</td>\n",
       "      <td>2023</td>\n",
       "      <td>44468.75</td>\n",
       "      <td>WCS</td>\n",
       "      <td>49.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>United States</td>\n",
       "      <td>USA</td>\n",
       "      <td>2023</td>\n",
       "      <td>65875.18</td>\n",
       "      <td>WCS</td>\n",
       "      <td>49.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     country name country code  year  gdp_per_capita type_  value\n",
       "0          Canada          CAN  2014        43643.24   WCS  65.69\n",
       "1   United States          USA  2014        55817.56   WCS  65.69\n",
       "2          Canada          CAN  2015        43594.19   WCS  30.43\n",
       "3   United States          USA  2015        57040.21   WCS  30.43\n",
       "4          Canada          CAN  2016        43551.34   WCS  17.88\n",
       "5   United States          USA  2016        57658.67   WCS  17.88\n",
       "6          Canada          CAN  2017        44339.39   WCS  37.19\n",
       "7   United States          USA  2017        58703.14   WCS  37.19\n",
       "8          Canada          CAN  2018        44907.34   WCS  42.53\n",
       "9   United States          USA  2018        60127.21   WCS  42.53\n",
       "10         Canada          CAN  2019        45100.29   WCS  34.30\n",
       "11  United States          USA  2019        61400.55   WCS  34.30\n",
       "12         Canada          CAN  2020        42366.13   WCS  36.82\n",
       "13  United States          USA  2020        59493.15   WCS  36.82\n",
       "14         Canada          CAN  2021        44359.62   WCS  40.04\n",
       "15  United States          USA  2021        62996.29   WCS  40.04\n",
       "16         Canada          CAN  2022        45227.14   WCS  65.60\n",
       "17  United States          USA  2022        64342.12   WCS  65.60\n",
       "18         Canada          CAN  2023        44468.75   WCS  49.94\n",
       "19  United States          USA  2023        65875.18   WCS  49.94"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country name</th>\n",
       "      <th>country code</th>\n",
       "      <th>year</th>\n",
       "      <th>gdp_per_capita</th>\n",
       "      <th>type_</th>\n",
       "      <th>wcs_percent_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Canada</td>\n",
       "      <td>CAN</td>\n",
       "      <td>2014</td>\n",
       "      <td>43643.24</td>\n",
       "      <td>WCS</td>\n",
       "      <td>65.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>United States</td>\n",
       "      <td>USA</td>\n",
       "      <td>2014</td>\n",
       "      <td>55817.56</td>\n",
       "      <td>WCS</td>\n",
       "      <td>65.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Canada</td>\n",
       "      <td>CAN</td>\n",
       "      <td>2015</td>\n",
       "      <td>43594.19</td>\n",
       "      <td>WCS</td>\n",
       "      <td>30.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>United States</td>\n",
       "      <td>USA</td>\n",
       "      <td>2015</td>\n",
       "      <td>57040.21</td>\n",
       "      <td>WCS</td>\n",
       "      <td>30.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Canada</td>\n",
       "      <td>CAN</td>\n",
       "      <td>2016</td>\n",
       "      <td>43551.34</td>\n",
       "      <td>WCS</td>\n",
       "      <td>17.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    country name country code  year  gdp_per_capita type_  wcs_percent_change\n",
       "0         Canada          CAN  2014        43643.24   WCS               65.69\n",
       "1  United States          USA  2014        55817.56   WCS               65.69\n",
       "2         Canada          CAN  2015        43594.19   WCS               30.43\n",
       "3  United States          USA  2015        57040.21   WCS               30.43\n",
       "4         Canada          CAN  2016        43551.34   WCS               17.88"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename \"value\" to \"wcs_percent_change\"\n",
    "merged_df.rename(columns={\"value\": \"wcs_percent_change\"}, inplace=True)\n",
    "\n",
    "# Display the updated column names\n",
    "merged_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: Trained and intiialized the model using Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (16, 5) (16,)\n",
      "Testing set: (4, 5) (4,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "df = merged_df.copy()\n",
    "X = df.drop(columns=['wcs_percent_change'])  # Features\n",
    "y = df['wcs_percent_change']  # Target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"Training set:\", X_train.shape, y_train.shape)\n",
    "print(\"Testing set:\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled Training set shape: (16, 4)\n",
      "Scaled Testing set shape: (4, 4)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming merged_df is the cleaned DataFrame\n",
    "df = merged_df.copy()\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = ['country name', 'country code', 'type_']\n",
    "\n",
    "# One-Hot Encode categorical columns\n",
    "df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Define Features (X) and Target (y)\n",
    "X = df.drop(columns=['wcs_percent_change'])  # Features\n",
    "y = df['wcs_percent_change']  # Target variable\n",
    "\n",
    "# Split the dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on training data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the test set\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Display the shape of transformed datasets\n",
    "print(\"Scaled Training set shape:\", X_train_scaled.shape)\n",
    "print(\"Scaled Testing set shape:\", X_test_scaled.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-01 13:56:22.861682: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/jsflora/Library/Python/3.12/lib/python/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,050</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m)             │           \u001b[38;5;34m400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │         \u001b[38;5;34m4,050\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m51\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,501</span> (17.58 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,501\u001b[0m (17.58 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,501</span> (17.58 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,501\u001b[0m (17.58 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Get number of input features (columns in X_train)\n",
    "number_input_features = X_train_scaled.shape[1]  # Use .shape[1] instead of len(X_train[0])\n",
    "\n",
    "# Define the number of nodes for each hidden layer\n",
    "hidden_nodes_layer1 = 80\n",
    "hidden_nodes_layer2 = 50\n",
    "\n",
    "# Initialize the neural network\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\"))\n",
    "\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))  # Use \"sigmoid\" for binary classification, \"linear\" for regression\n",
    "\n",
    "# Check model architecture\n",
    "nn.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.0000e+00 - loss: 1.6801\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.0000e+00 - loss: -0.8122\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - accuracy: 0.0000e+00 - loss: -3.2469\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.0000e+00 - loss: -5.6664\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.0000e+00 - loss: -8.0409\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.0000e+00 - loss: -10.4021\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.0000e+00 - loss: -12.7876\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.0000e+00 - loss: -15.1971\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.0000e+00 - loss: -17.5936\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.0000e+00 - loss: -19.9695\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "fit_model = nn.fit(X_train_scaled, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 - 0s - 230ms/step - accuracy: 0.0000e+00 - loss: -1.9824e+01\n",
      "Loss: -19.82416534423828, Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3: Retriveing the Model using Spark and model optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: apt-get\n",
      "zsh:1: command not found: apt-get\n",
      "zsh:1: command not found: wget\n",
      "tar: Error opening archive: Failed to open 'spark-3.5.1-bin-hadoop3.tgz'\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Unable to find py4j in /content/spark-3.5.1-bin-hadoop3/python, your SPARK_HOME may not be configured correctly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/findspark.py:159\u001b[0m, in \u001b[0;36minit\u001b[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     py4j \u001b[38;5;241m=\u001b[39m \u001b[43mglob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark_python\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlib\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpy4j-*.zip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfindspark\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Initialize findspark with the correct SPARK_HOME path\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[43mfindspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/content/spark-3.5.1-bin-hadoop3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Create a Spark session\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/findspark.py:161\u001b[0m, in \u001b[0;36minit\u001b[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[1;32m    159\u001b[0m         py4j \u001b[38;5;241m=\u001b[39m glob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(spark_python, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlib\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpy4j-*.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m))[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[1;32m    162\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to find py4j in \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, your SPARK_HOME may not be configured correctly\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    163\u001b[0m                 spark_python\n\u001b[1;32m    164\u001b[0m             )\n\u001b[1;32m    165\u001b[0m         )\n\u001b[1;32m    166\u001b[0m     sys\u001b[38;5;241m.\u001b[39mpath[:\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m sys_path \u001b[38;5;241m=\u001b[39m [spark_python, py4j]\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# already imported, no need to patch sys.path\u001b[39;00m\n",
      "\u001b[0;31mException\u001b[0m: Unable to find py4j in /content/spark-3.5.1-bin-hadoop3/python, your SPARK_HOME may not be configured correctly"
     ]
    }
   ],
   "source": [
    "# Update package lists and install Java\n",
    "!apt-get update\n",
    "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
    "\n",
    "# Download Spark 3.5.1 with Hadoop 3\n",
    "!wget -q https://downloads.apache.org/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz\n",
    "\n",
    "# Extract Spark\n",
    "!tar -xvf spark-3.5.1-bin-hadoop3.tgz\n",
    "import os\n",
    "\n",
    "# Set environment variables for Spark\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.1-bin-hadoop3\"\n",
    "!pip install -q findspark\n",
    "import findspark\n",
    "\n",
    "# Initialize findspark with the correct SPARK_HOME path\n",
    "findspark.init(\"/content/spark-3.5.1-bin-hadoop3\")\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"WSC_Model\").getOrCreate()\n",
    "\n",
    "# Check Spark version\n",
    "print(f\"Spark Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"WSC_Model\").getOrCreate()\n",
    "\n",
    "# Check Spark version\n",
    "print(f\"Spark Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder.appName(\"WSC_Model\").getOrCreate()\n",
    "\n",
    "# Load data into Spark DataFrame\n",
    "spark_df = spark.read.csv(\"merged_df.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Convert Spark DataFrame to Pandas for TensorFlow training\n",
    "df = spark_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# One-hot encode categorical columns\n",
    "df = pd.get_dummies(df, columns=['country name', 'country code', 'type_'], drop_first=True)\n",
    "\n",
    "# Define Features and Target\n",
    "X = df.drop(columns=['value'])  # Predicting 'value' (WSC price)\n",
    "y = df['value']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardization\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the number of input features\n",
    "number_input_features = X_train_scaled.shape[1]\n",
    "\n",
    "# Initialize the model\n",
    "nn = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(units=80, input_dim=number_input_features, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(units=50, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(units=1, activation=\"linear\")  # Linear for regression\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "nn.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[\"mae\"])\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model and log results\n",
    "history = nn.fit(X_train_scaled, y_train, \n",
    "                 validation_data=(X_test_scaled, y_test), \n",
    "                 epochs=100, batch_size=16, \n",
    "                 callbacks=[early_stopping], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Evaluate on test data\n",
    "y_pred = nn.predict(X_test_scaled)\n",
    "\n",
    "# Compute R-squared\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "loss, mae = nn.evaluate(X_test_scaled, y_test)\n",
    "print(f\"\\n Model Performance \\nR² Score: {r2:.2f} \\nTest Loss: {loss:.4f} \\nTest MAE: {mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 4: Display the Models Overall Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Save model results in CSV\n",
    "model_results = [\n",
    "    [\"Experiment\", \"Hidden Layer 1\", \"Hidden Layer 2\", \"Batch Size\", \"Epochs\", \"Validation Loss\", \"Test MAE\", \"R² Score\"],\n",
    "    [1, 80, 50, 16, len(history.history['loss']), min(history.history['val_loss']), mae, r2]\n",
    "]\n",
    "\n",
    "# Write to CSV\n",
    "with open(\"model_optimization_log.csv\", \"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(model_results)\n",
    "\n",
    "print(\"\\n🔍 Model results saved in 'model_optimization_log.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
